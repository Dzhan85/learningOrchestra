{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is? learningOrchestra The goal of this work is to develop a tool, named Learning Orchestra, to reduce a bit more the existing gap in facilitate and streamline the data scientist iterative process composed of gather data, clean/prepare those data, build models, validate their predictions and deploy the results. The learningOrchestra use microservices in a cluster, is possible load a dataset in csv format from an URL using the database api microservice, this csv file is converted to json file to be stored in MongoDB, also is possible perform several preprocessing and analytical tasks using this microservices . The main feature of learningOrchestra is make prediction models with different classificators simultaneously using stored and preprocessed datasets with model builder microservice, this microservice use a spark cluster to make prediction models using distributed processing. You can compare the different classificators result as time to fit and prediction accuracy, the fact of the user usage your own preprocessing code allow the creation of highly customized model predictions to a specific dataset, increasing the accuracy and results, the sky is the limit! \ud83d\ude80\ud83d\ude80 To turn the learningOrchestra use more easy, there is the learning_orchestra_client python package, this package provide to an user all learningOrchestra functionalities in coding way, to improve your user experience you can export and analyse the results using a GUI of MongoDB as NoSQLBooster , also there is an example of usage of learningOrchestra with the titanic challenge dataset , each microservice and python package has the own documentation with examples of use, more details in below. Architecture","title":"Whats is?"},{"location":"#what-is","text":"","title":"What is?"},{"location":"#learningorchestra","text":"The goal of this work is to develop a tool, named Learning Orchestra, to reduce a bit more the existing gap in facilitate and streamline the data scientist iterative process composed of gather data, clean/prepare those data, build models, validate their predictions and deploy the results. The learningOrchestra use microservices in a cluster, is possible load a dataset in csv format from an URL using the database api microservice, this csv file is converted to json file to be stored in MongoDB, also is possible perform several preprocessing and analytical tasks using this microservices . The main feature of learningOrchestra is make prediction models with different classificators simultaneously using stored and preprocessed datasets with model builder microservice, this microservice use a spark cluster to make prediction models using distributed processing. You can compare the different classificators result as time to fit and prediction accuracy, the fact of the user usage your own preprocessing code allow the creation of highly customized model predictions to a specific dataset, increasing the accuracy and results, the sky is the limit! \ud83d\ude80\ud83d\ude80 To turn the learningOrchestra use more easy, there is the learning_orchestra_client python package, this package provide to an user all learningOrchestra functionalities in coding way, to improve your user experience you can export and analyse the results using a GUI of MongoDB as NoSQLBooster , also there is an example of usage of learningOrchestra with the titanic challenge dataset , each microservice and python package has the own documentation with examples of use, more details in below.","title":"learningOrchestra"},{"location":"#architecture","text":"","title":"Architecture"},{"location":"data_type_handler/","text":"Data Type Handler microservice Microservice used to change data type from stored file between number and string. Change fields type of inserted file PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request use filename as id in argument and fields in body, fields are an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Data Type Handler"},{"location":"data_type_handler/#data-type-handler-microservice","text":"Microservice used to change data type from stored file between number and string.","title":"Data Type Handler microservice"},{"location":"data_type_handler/#change-fields-type-of-inserted-file","text":"PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request use filename as id in argument and fields in body, fields are an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Change fields type of inserted file"},{"location":"database_api/","text":"Database API microservice The Database API microservice create a level of abstraction through an REST API to use the database, datasets are downloaded in csv and handled in json format, the primary key for each document is the filename field contained in the sent json file from POST request. GUI tool to handle database files There are GUI tools to handle database files, as example, the NoSQLBooster can interact with mongoDB used in database, and make several tasks which are limited in learning_orchestra_client package, as schema visualization and files extraction and download to formats as csv, json, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool, connect with the url cluster_ip:27017 and use the user root with password owl45#21. List all inserted files GET CLUSTER_IP:5000/files Return an array of metadata files in database, each file inserted in database contains a metadata file. Downloaded files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - column names from inserted file filename - name to file identification finished - flag used to indicate if asyncronous processing from file downloader is finished time_created - creation time of file url - url used to file download Preprocessed files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - file filename used to make preprocessing operation Classificator prediction files metadata { \"accuracy\" : \"1.0\" , \"classificator\" : \"gb\" , \"error\" : \"0.0\" , \"filename\" : \"titanic_testing_900_prediction_gb\" , \"fit_time\" : 69.43671989440918 } accuracy - accuracy rate from model prediction classificator - initials from used classificator error - error rate from model prediction fit_time - time from model fit using training dataset List file content GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Return rows of filename, and paginate in query result. filename - filename of inserted file skip - amount lines to skip in csv file limit - limit of returned query result, max limit setted in 20 rows query - query to find documents, if use method only to paginate, use blank json, as {} The first row is always the metadata file. Insert file from URL POST CLUSTER_IP:5000/files Insert a csv into the database using the POST method, json must be contained in the body of the http request. The inserted json must has the fields: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" } Delete inserted file DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE, informing the value of filename field of a inserted file in argument request, deleting the database file, if one exist with that value.","title":"Database API"},{"location":"database_api/#database-api-microservice","text":"The Database API microservice create a level of abstraction through an REST API to use the database, datasets are downloaded in csv and handled in json format, the primary key for each document is the filename field contained in the sent json file from POST request.","title":"Database API microservice"},{"location":"database_api/#gui-tool-to-handle-database-files","text":"There are GUI tools to handle database files, as example, the NoSQLBooster can interact with mongoDB used in database, and make several tasks which are limited in learning_orchestra_client package, as schema visualization and files extraction and download to formats as csv, json, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool, connect with the url cluster_ip:27017 and use the user root with password owl45#21.","title":"GUI tool to handle database files"},{"location":"database_api/#list-all-inserted-files","text":"GET CLUSTER_IP:5000/files Return an array of metadata files in database, each file inserted in database contains a metadata file.","title":"List all inserted files"},{"location":"database_api/#downloaded-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - column names from inserted file filename - name to file identification finished - flag used to indicate if asyncronous processing from file downloader is finished time_created - creation time of file url - url used to file download","title":"Downloaded files metadata"},{"location":"database_api/#preprocessed-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - file filename used to make preprocessing operation","title":"Preprocessed files metadata"},{"location":"database_api/#classificator-prediction-files-metadata","text":"{ \"accuracy\" : \"1.0\" , \"classificator\" : \"gb\" , \"error\" : \"0.0\" , \"filename\" : \"titanic_testing_900_prediction_gb\" , \"fit_time\" : 69.43671989440918 } accuracy - accuracy rate from model prediction classificator - initials from used classificator error - error rate from model prediction fit_time - time from model fit using training dataset","title":"Classificator prediction files metadata"},{"location":"database_api/#list-file-content","text":"GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Return rows of filename, and paginate in query result. filename - filename of inserted file skip - amount lines to skip in csv file limit - limit of returned query result, max limit setted in 20 rows query - query to find documents, if use method only to paginate, use blank json, as {} The first row is always the metadata file.","title":"List file content"},{"location":"database_api/#insert-file-from-url","text":"POST CLUSTER_IP:5000/files Insert a csv into the database using the POST method, json must be contained in the body of the http request. The inserted json must has the fields: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" }","title":"Insert file from URL"},{"location":"database_api/#delete-inserted-file","text":"DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE, informing the value of filename field of a inserted file in argument request, deleting the database file, if one exist with that value.","title":"Delete inserted file"},{"location":"histogram/","text":"Histogram microservice Microservice used to make the histogram from stored file, storing the result in a new file in MongoDB. Create an Histogram from inserted file POST CLUSTER_IP:5004/histograms/<filename> The request is sent in body, histrogram_filename is the filename to save the histogram result and fields are an array whith all fields to be maked the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Histogram"},{"location":"histogram/#histogram-microservice","text":"Microservice used to make the histogram from stored file, storing the result in a new file in MongoDB.","title":"Histogram microservice"},{"location":"histogram/#create-an-histogram-from-inserted-file","text":"POST CLUSTER_IP:5004/histograms/<filename> The request is sent in body, histrogram_filename is the filename to save the histogram result and fields are an array whith all fields to be maked the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Create an Histogram from inserted file"},{"location":"install/","text":"Install Requirements Linux hosts Docker Engine installed in all instances of your cluster cluster configured in swarm mode, more details in swarm documentation Docker Compose installed in manager instance of your cluster Ensure wich your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration. Deploy Ensure wich you location path is in project root (./learningOrchestra), in sequence, run the command bellow in manager instance of swarm cluster to deploy the learningOrchestra: sudo ./run.sh If all things happen good, the learningOrchestra has been deployed in your swarm cluster, congrulations! \ud83e\udd73 \ud83d\udc4f\ud83d\udc4f learningOrchestra cluster state There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:80 Visualize spark cluster state - CLUSTER_IP:8080","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#requirements","text":"Linux hosts Docker Engine installed in all instances of your cluster cluster configured in swarm mode, more details in swarm documentation Docker Compose installed in manager instance of your cluster Ensure wich your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration.","title":"Requirements"},{"location":"install/#deploy","text":"Ensure wich you location path is in project root (./learningOrchestra), in sequence, run the command bellow in manager instance of swarm cluster to deploy the learningOrchestra: sudo ./run.sh If all things happen good, the learningOrchestra has been deployed in your swarm cluster, congrulations! \ud83e\udd73 \ud83d\udc4f\ud83d\udc4f","title":"Deploy"},{"location":"install/#learningorchestra-cluster-state","text":"There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:80 Visualize spark cluster state - CLUSTER_IP:8080","title":"learningOrchestra cluster state"},{"location":"learning_orchestra_client_package/","text":"learningOrchestra client package You can know how install and use this package at learning_orchestra_client repositoy .","title":"Python Package"},{"location":"learning_orchestra_client_package/#learningorchestra-client-package","text":"You can know how install and use this package at learning_orchestra_client repositoy .","title":"learningOrchestra client package"},{"location":"model_builder/","text":"Model builder microservice Model Builder microservice provide a REST API to create several model predictions using your own preprocessing code using a defined set of classificators. Create prediction model POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" } classificators_list \"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] } preprocessor_code environment The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance for training filename testing_df (Instantiated): Spark Dataframe instance for testing filename The preprocessing code must instantiate the variables in bellow, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None. Handy methods self . fields_from_dataframe ( self , dataframe , is_string ) This method return string or number fields as string list from a dataframe. dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields. preprocessor_code example This example use the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Ticket\" , \"Cabin\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [ 1 :], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.1 , 0.9 ], seed = 11 ) features_testing = assembler . transform ( testing_df )","title":"Model Builder"},{"location":"model_builder/#model-builder-microservice","text":"Model Builder microservice provide a REST API to create several model predictions using your own preprocessing code using a defined set of classificators.","title":"Model builder microservice"},{"location":"model_builder/#create-prediction-model","text":"POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" }","title":"Create prediction model"},{"location":"model_builder/#classificators_list","text":"\"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] }","title":"classificators_list"},{"location":"model_builder/#preprocessor_code-environment","text":"The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance for training filename testing_df (Instantiated): Spark Dataframe instance for testing filename The preprocessing code must instantiate the variables in bellow, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None.","title":"preprocessor_code environment"},{"location":"model_builder/#handy-methods","text":"self . fields_from_dataframe ( self , dataframe , is_string ) This method return string or number fields as string list from a dataframe. dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields.","title":"Handy methods"},{"location":"model_builder/#preprocessor_code-example","text":"This example use the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Ticket\" , \"Cabin\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [ 1 :], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.1 , 0.9 ], seed = 11 ) features_testing = assembler . transform ( testing_df )","title":"preprocessor_code example"},{"location":"pca/","text":"PCA microservice PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs . Create an image plot POST CLUSTER_IP:5006/images/<parent_filename> The request use a parent_filename as a dataset inserted filename, the body contains the json fields: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The \"label_name\" is the label name column for machine learning datasets which has labeled tuples, case of the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : null } Delete an image plot DELETE CLUSTER_IP:5006/images/<image_plot_filename> Read the filenames of the created images GET CLUSTER_IP:5006/images This request return a list with all created images plot filenames. Read an image plot GET CLUSTER_IP:5006/images/<image_plot_filename> This request return return the image plot of filename. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"PCA"},{"location":"pca/#pca-microservice","text":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs .","title":"PCA microservice"},{"location":"pca/#create-an-image-plot","text":"POST CLUSTER_IP:5006/images/<parent_filename> The request use a parent_filename as a dataset inserted filename, the body contains the json fields: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The \"label_name\" is the label name column for machine learning datasets which has labeled tuples, case of the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : null }","title":"Create an image plot"},{"location":"pca/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5006/images/<image_plot_filename>","title":"Delete an image plot"},{"location":"pca/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5006/images This request return a list with all created images plot filenames.","title":"Read the filenames of the created images"},{"location":"pca/#read-an-image-plot","text":"GET CLUSTER_IP:5006/images/<image_plot_filename> This request return return the image plot of filename.","title":"Read an image plot"},{"location":"pca/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"pca/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"pca/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"projection/","text":"Projection microservice Projection microservice provide an api to make a projection from file inserted in database service, generating a new file and putting in database. Create projection from inserted file POST CLUSTER_IP:5001/projections/<filename> { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Projection"},{"location":"projection/#projection-microservice","text":"Projection microservice provide an api to make a projection from file inserted in database service, generating a new file and putting in database.","title":"Projection microservice"},{"location":"projection/#create-projection-from-inserted-file","text":"POST CLUSTER_IP:5001/projections/<filename> { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Create projection from inserted file"},{"location":"t_sne/","text":"t-SNE microservice The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in wikipedia . Create an image plot POST CLUSTER_IP:5005/images/<parent_filename> The request use a parent_filename as a dataset inserted filename, the body contains the json fields: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The \"label_name\" is the label name column for machine learning datasets which has labeled tuples, case of the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : null } Delete an image plot DELETE CLUSTER_IP:5005/images/<image_plot_filename> Read the filenames of the created images GET CLUSTER_IP:5005/images This request return a list with all created images plot filenames. Read an image plot GET CLUSTER_IP:5005/images/<image_plot_filename> This request return return the image plot of filename. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"t-SNE"},{"location":"t_sne/#t-sne-microservice","text":"The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in wikipedia .","title":"t-SNE microservice"},{"location":"t_sne/#create-an-image-plot","text":"POST CLUSTER_IP:5005/images/<parent_filename> The request use a parent_filename as a dataset inserted filename, the body contains the json fields: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The \"label_name\" is the label name column for machine learning datasets which has labeled tuples, case of the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : null }","title":"Create an image plot"},{"location":"t_sne/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5005/images/<image_plot_filename>","title":"Delete an image plot"},{"location":"t_sne/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5005/images This request return a list with all created images plot filenames.","title":"Read the filenames of the created images"},{"location":"t_sne/#read-an-image-plot","text":"GET CLUSTER_IP:5005/images/<image_plot_filename> This request return return the image plot of filename.","title":"Read an image plot"},{"location":"t_sne/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"t_sne/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"t_sne/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"usage/","text":"Usage Python package learningOrchestra Client - The python package for learningOrchestra use microservices REST API database api - Microservice used to download and handling files in database projection - Make projections of stored files in database using spark cluster data type handler - Change fields file type between number and text histogram - Make histograms of stored files in database t-SNE - Make a t-SNE image plot of stored files in database PCA - Make a PCA image plot of stored files in database model builder - Create a prediction model from preprocessed files using spark cluster database GUI NoSQLBooster - MongoDB GUI to make several database tasks, as files visualization, querys, projections and files extraction to formats as csv and json, read the database api docs to learn how configure this tool.","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#python-package","text":"learningOrchestra Client - The python package for learningOrchestra use","title":"Python package"},{"location":"usage/#microservices-rest-api","text":"database api - Microservice used to download and handling files in database projection - Make projections of stored files in database using spark cluster data type handler - Change fields file type between number and text histogram - Make histograms of stored files in database t-SNE - Make a t-SNE image plot of stored files in database PCA - Make a PCA image plot of stored files in database model builder - Create a prediction model from preprocessed files using spark cluster","title":"microservices REST API"},{"location":"usage/#database-gui","text":"NoSQLBooster - MongoDB GUI to make several database tasks, as files visualization, querys, projections and files extraction to formats as csv and json, read the database api docs to learn how configure this tool.","title":"database GUI"}]}