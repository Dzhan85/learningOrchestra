{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is? learningOrchestra The learningOrchestra is a software for distributed machine learning processing using microservices in a cluster, is possible load a csv file from an URL using the database api microservice, this csv file is converted to json file to be stored in MongoDB, also is possible make preprocessing tasks using microservices as projection , data type handler and histogram . The main feature of learningOrchestra is make prediction models with different classificators simultaneously using stored and preprocessed datasets with model builder microservice, this microservice use a spark cluster to make prediction models using distributed processing. You can compare the differents classificators result as time to fit and prediction accuracy, the fact of the user usage your own preprocessing code allow the creation of highly customized model predictons to a specific dataset, increasing the accuracy and results, the sky is the limit! \ud83d\ude80\ud83d\ude80 To turn the learningOrchestra use more easy, there is the learning_orchestra_client python package, this package provide to an user all learningOrchestra functionalities in coding way, to improve your user experience you can export and analyse the results using a GUI of MongoDB as NoSQLBooster , also there is an example of usage of learningOrchestra with the titanic challenge dataset , each microservice and python package has the own documentation with examples of use, more details in below. Architecture","title":"Whats is?"},{"location":"#what-is","text":"","title":"What is?"},{"location":"#learningorchestra","text":"The learningOrchestra is a software for distributed machine learning processing using microservices in a cluster, is possible load a csv file from an URL using the database api microservice, this csv file is converted to json file to be stored in MongoDB, also is possible make preprocessing tasks using microservices as projection , data type handler and histogram . The main feature of learningOrchestra is make prediction models with different classificators simultaneously using stored and preprocessed datasets with model builder microservice, this microservice use a spark cluster to make prediction models using distributed processing. You can compare the differents classificators result as time to fit and prediction accuracy, the fact of the user usage your own preprocessing code allow the creation of highly customized model predictons to a specific dataset, increasing the accuracy and results, the sky is the limit! \ud83d\ude80\ud83d\ude80 To turn the learningOrchestra use more easy, there is the learning_orchestra_client python package, this package provide to an user all learningOrchestra functionalities in coding way, to improve your user experience you can export and analyse the results using a GUI of MongoDB as NoSQLBooster , also there is an example of usage of learningOrchestra with the titanic challenge dataset , each microservice and python package has the own documentation with examples of use, more details in below.","title":"learningOrchestra"},{"location":"#architecture","text":"","title":"Architecture"},{"location":"data_type_handler/","text":"Data Type Handler microservice Microservice used to change data type from stored file between number and string Change fields type of inserted file PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request use filename as id in argument and fields in body, fields are an array whith all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Data Type Handler"},{"location":"data_type_handler/#data-type-handler-microservice","text":"Microservice used to change data type from stored file between number and string","title":"Data Type Handler microservice"},{"location":"data_type_handler/#change-fields-type-of-inserted-file","text":"PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request use filename as id in argument and fields in body, fields are an array whith all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Change fields type of inserted file"},{"location":"database_api/","text":"Database API microservice The Database API microservice create a level of abstraction through an REST API to use the database, datasets are downloaded in csv and handled in json format, the primary key for each document is the filename field contained in the sent json file from POST request. GUI tool to handle database files There are GUI tools to handle database files, as example, the NoSQLBooster can interact with mongoDB used in database, and make several tasks which are limited in learning_orchestra_client package, as schema visualization and files extraction and download to formats as csv, json, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool, connect with the url cluster_ip:27017 and use the user root with password owl45#21. List all inserted files GET CLUSTER_IP:5000/files Return an array of metadata files in database, each file inserted in database contains a metadata file. Downloaded files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - column names from inserted file filename - name to file identification finished - flag used to indicate if asyncronous processing from file downloader is finished time_created - creation time of file url - url used to file download Preprocessed files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - file filename used to make preprocessing operation Classificator prediction files metadata { \"accuracy\" : \"1.0\" , \"classificator\" : \"gb\" , \"error\" : \"0.0\" , \"filename\" : \"titanic_testing_900_prediction_gb\" , \"fit_time\" : 69.43671989440918 } accuracy - accuracy rate from model prediction classificator - initials from used classificator error - error rate from model prediction fit_time - time from model fit using training dataset List file content GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Return rows of filename, and paginate in query result filename - filename of inserted file skip - amount lines to skip in csv file limit - limit of returned query result, max limit setted in 20 rows query - query to find documents, if use method only to paginate, use blank json, as {} The first row is always the metadata file Insert file from URL POST CLUSTER_IP:5000/files Insert a csv into the database using the POST method, json must be contained in the body of the http request. The inserted json must has the fields: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" } Delete inserted file DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE, informing the value of filename field of a inserted file in argument request, deleting the database file, if one exist with that value.","title":"Database API"},{"location":"database_api/#database-api-microservice","text":"The Database API microservice create a level of abstraction through an REST API to use the database, datasets are downloaded in csv and handled in json format, the primary key for each document is the filename field contained in the sent json file from POST request.","title":"Database API microservice"},{"location":"database_api/#gui-tool-to-handle-database-files","text":"There are GUI tools to handle database files, as example, the NoSQLBooster can interact with mongoDB used in database, and make several tasks which are limited in learning_orchestra_client package, as schema visualization and files extraction and download to formats as csv, json, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool, connect with the url cluster_ip:27017 and use the user root with password owl45#21.","title":"GUI tool to handle database files"},{"location":"database_api/#list-all-inserted-files","text":"GET CLUSTER_IP:5000/files Return an array of metadata files in database, each file inserted in database contains a metadata file.","title":"List all inserted files"},{"location":"database_api/#downloaded-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - column names from inserted file filename - name to file identification finished - flag used to indicate if asyncronous processing from file downloader is finished time_created - creation time of file url - url used to file download","title":"Downloaded files metadata"},{"location":"database_api/#preprocessed-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - file filename used to make preprocessing operation","title":"Preprocessed files metadata"},{"location":"database_api/#classificator-prediction-files-metadata","text":"{ \"accuracy\" : \"1.0\" , \"classificator\" : \"gb\" , \"error\" : \"0.0\" , \"filename\" : \"titanic_testing_900_prediction_gb\" , \"fit_time\" : 69.43671989440918 } accuracy - accuracy rate from model prediction classificator - initials from used classificator error - error rate from model prediction fit_time - time from model fit using training dataset","title":"Classificator prediction files metadata"},{"location":"database_api/#list-file-content","text":"GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Return rows of filename, and paginate in query result filename - filename of inserted file skip - amount lines to skip in csv file limit - limit of returned query result, max limit setted in 20 rows query - query to find documents, if use method only to paginate, use blank json, as {} The first row is always the metadata file","title":"List file content"},{"location":"database_api/#insert-file-from-url","text":"POST CLUSTER_IP:5000/files Insert a csv into the database using the POST method, json must be contained in the body of the http request. The inserted json must has the fields: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" }","title":"Insert file from URL"},{"location":"database_api/#delete-inserted-file","text":"DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE, informing the value of filename field of a inserted file in argument request, deleting the database file, if one exist with that value.","title":"Delete inserted file"},{"location":"histogram/","text":"Histogram microservice Microservice used to make the histogram from stored file, storing the result in a new file in MongoDB. Create an Histogram from inserted file POST CLUSTER_IP:5004/histograms/<filename> The request is sent in body, histrogram_filename is the filename to save the histogram result and fields are an array whith all fields to be maked the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Histogram"},{"location":"histogram/#histogram-microservice","text":"Microservice used to make the histogram from stored file, storing the result in a new file in MongoDB.","title":"Histogram microservice"},{"location":"histogram/#create-an-histogram-from-inserted-file","text":"POST CLUSTER_IP:5004/histograms/<filename> The request is sent in body, histrogram_filename is the filename to save the histogram result and fields are an array whith all fields to be maked the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Create an Histogram from inserted file"},{"location":"install/","text":"Install Requirements Linux hosts Docker Engine installed in all instances of your cluster cluster configured in swarm mode, more details in swarm documentation Docker Compose installed in manager instance of your cluster Ensure wich your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration. Deploy Ensure wich you location path is in project root (./learningOrchestra), in sequence, run the command bellow in manager instance of swarm cluster to deploy the learningOrchestra: sudo ./run.sh If all things happen good, the learningOrchestra has been deployed in your swarm cluster, congrulations! \ud83e\udd73 \ud83d\udc4f\ud83d\udc4f learningOrchestra cluster state There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:8000 Visualize spark cluster state - CLUSTER_IP:8080","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#requirements","text":"Linux hosts Docker Engine installed in all instances of your cluster cluster configured in swarm mode, more details in swarm documentation Docker Compose installed in manager instance of your cluster Ensure wich your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration.","title":"Requirements"},{"location":"install/#deploy","text":"Ensure wich you location path is in project root (./learningOrchestra), in sequence, run the command bellow in manager instance of swarm cluster to deploy the learningOrchestra: sudo ./run.sh If all things happen good, the learningOrchestra has been deployed in your swarm cluster, congrulations! \ud83e\udd73 \ud83d\udc4f\ud83d\udc4f","title":"Deploy"},{"location":"install/#learningorchestra-cluster-state","text":"There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:8000 Visualize spark cluster state - CLUSTER_IP:8080","title":"learningOrchestra cluster state"},{"location":"learning_orchestra_client_package/","text":"learningOrchestra client package Installation Ensure which you have the python 3 installed in your machine and run: pip install learning_orchestra_cliet Documentation After downloading the package, import all classes: from learning_orchestra_client import * create a Context object passing a ip from your cluster in constructor parameter: cluster_ip = \"34.95.222.197\" Context ( cluster_ip ) After create a Context object, you will able to usage learningOrchestra, each learningOrchestra funcionalite is contained in your own class, therefore, to use a specific funcionalite, after you instanciate and configure Context class, you need instanciate and call the method class of interest, in below, there are all class and each class methods, also have an example of workflow using this package in a python code. DatabaseApi read_resume_files read_resume_files ( pretty_response = True ) Read all metadata files in learningOrchestra * pretty_response: return indented string to visualization (default True, if False, return dict) read_file read_file ( self , filename_key , skip = 0 , limit = 10 , query = {}, pretty_response = True ) filename_ley : filename of file skip: number of rows amount to skip in pagination (default 0) limit: number of rows to return in pagination (default 10)(max setted in 20 rows per request) query: query to make in mongo (default empty query) pretty_response: return indented string to visualization (default True, if False, return dict) create_file create_file ( self , filename , url , pretty_response = True ) filename: filename of file to be created url: url to csv file pretty_response: return indented string to visualization (default True, if False, return dict) delete_file delete_file ( self , filename , pretty_response = True ) filename: file filename to be deleted pretty_response: return indented string to visualization (default True, if False, return dict) Projection create_projection create_projection ( self , filename , projection_filename , fields , pretty_response = True ) filename: filename of file to make projection projection_filename: filename used to create projection field: list with fields to make projection pretty_response: return indented string to visualization (default True, if False, return dict) DataTypeHandler change_file_type change_file_type ( self , filename , fields_dict , pretty_response = True ) filename: filename of file fields_dict: dictionary with \"field\": \"number\" or field: \"string\" keys pretty_response: return indented string to visualization (default True, if False, return dict) Histogram create_histogram def create_histogram ( self , filename , histogram_filename , fields , pretty_response = True ) filename: filename of file to make histogram histogram_filename: filename used to create histogram fields: list with fields to make histogram pretty_response: return indented string to visualization (default True, if False, return dict) ModelBuilder create_model create_model ( self , training_filename , test_filename , preprocessor_code , model_classificator , pretty_response = True ) training_filename: filename to be used in training test_filename: filename to be used in test preprocessor_code: python3 code for pyspark preprocessing model model_classificator: list of initial from classificators to be used in model pretty_response: return indented string to visualization (default True, if False, return dict) model_classificator \"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: create_model ( training_filename , test_filename , preprocessor_code , [ \"lr\" , \"nb\" ]) preprocessor_code environment The python 3 preprocessing code must use the environment instances in bellow: training_df (Instanciated): Spark Dataframe instance for trainingfilename testing_df (Instanciated): Spark Dataframe instance for testing filename The preprocessing code must instanciate the variables in bellow, , all intances must be transformed by pyspark VectorAssembler: features_training (Not Instanciated): Spark Dataframe instance for train the model features_evaluation (Not Instanciated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instanciated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None. Handy methods self . fields_from_dataframe ( self , dataframe , is_string ) dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields. learning_orchestra_client usage example In below there is a python script using the package: from learning_orchestra_client import * cluster_ip = \"34.95.187.26\" Context ( cluster_ip ) database_api = DatabaseApi () print ( database_api . create_file ( \"titanic_training\" , \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" )) print ( database_api . create_file ( \"titanic_testing\" , \"https://filebin.net/mguee52ke97k0x9h/titanic_testing.csv?t=ub4nc1rc\" )) print ( database_api . read_resume_files ()) data_type_handler = DataTypeHandler () print ( data_type_handler . change_file_type ( \"titanic_training\" , { \"Age\" : \"number\" , \"Fare\" : \"number\" , \"Parch\" : \"number\" , \"PassengerId\" : \"number\" , \"Pclass\" : \"number\" , \"SibSp\" : \"number\" , \"Survived\" : \"number\" })) print ( data_type_handler . change_file_type ( \"titanic_testing\" , { \"Age\" : \"number\" , \"Fare\" : \"number\" , \"Parch\" : \"number\" , \"PassengerId\" : \"number\" , \"Pclass\" : \"number\" , \"SibSp\" : \"number\" })) preprocessing_code = ''' from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = ['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don'] correct_initials = ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\"). \\ fit(dataset). \\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Ticket\", \"Cabin\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[1:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) = \\ features_training.randomSplit([0.1, 0.9], seed=11) features_testing = assembler.transform(testing_df) ''' model_builder = Model () print ( model_builder . create_model ( \"titanic_training\" , \"titanic_testing\" , preprocessing_code , [ \"lr\" , \"dt\" , \"gb\" , \"rf\" , \"nb\" ))","title":"Python Package"},{"location":"learning_orchestra_client_package/#learningorchestra-client-package","text":"","title":"learningOrchestra client package"},{"location":"learning_orchestra_client_package/#installation","text":"Ensure which you have the python 3 installed in your machine and run: pip install learning_orchestra_cliet","title":"Installation"},{"location":"learning_orchestra_client_package/#documentation","text":"After downloading the package, import all classes: from learning_orchestra_client import * create a Context object passing a ip from your cluster in constructor parameter: cluster_ip = \"34.95.222.197\" Context ( cluster_ip ) After create a Context object, you will able to usage learningOrchestra, each learningOrchestra funcionalite is contained in your own class, therefore, to use a specific funcionalite, after you instanciate and configure Context class, you need instanciate and call the method class of interest, in below, there are all class and each class methods, also have an example of workflow using this package in a python code.","title":"Documentation"},{"location":"learning_orchestra_client_package/#databaseapi","text":"","title":"DatabaseApi"},{"location":"learning_orchestra_client_package/#read_resume_files","text":"read_resume_files ( pretty_response = True ) Read all metadata files in learningOrchestra * pretty_response: return indented string to visualization (default True, if False, return dict)","title":"read_resume_files"},{"location":"learning_orchestra_client_package/#read_file","text":"read_file ( self , filename_key , skip = 0 , limit = 10 , query = {}, pretty_response = True ) filename_ley : filename of file skip: number of rows amount to skip in pagination (default 0) limit: number of rows to return in pagination (default 10)(max setted in 20 rows per request) query: query to make in mongo (default empty query) pretty_response: return indented string to visualization (default True, if False, return dict)","title":"read_file"},{"location":"learning_orchestra_client_package/#create_file","text":"create_file ( self , filename , url , pretty_response = True ) filename: filename of file to be created url: url to csv file pretty_response: return indented string to visualization (default True, if False, return dict)","title":"create_file"},{"location":"learning_orchestra_client_package/#delete_file","text":"delete_file ( self , filename , pretty_response = True ) filename: file filename to be deleted pretty_response: return indented string to visualization (default True, if False, return dict)","title":"delete_file"},{"location":"learning_orchestra_client_package/#projection","text":"","title":"Projection"},{"location":"learning_orchestra_client_package/#create_projection","text":"create_projection ( self , filename , projection_filename , fields , pretty_response = True ) filename: filename of file to make projection projection_filename: filename used to create projection field: list with fields to make projection pretty_response: return indented string to visualization (default True, if False, return dict)","title":"create_projection"},{"location":"learning_orchestra_client_package/#datatypehandler","text":"","title":"DataTypeHandler"},{"location":"learning_orchestra_client_package/#change_file_type","text":"change_file_type ( self , filename , fields_dict , pretty_response = True ) filename: filename of file fields_dict: dictionary with \"field\": \"number\" or field: \"string\" keys pretty_response: return indented string to visualization (default True, if False, return dict)","title":"change_file_type"},{"location":"learning_orchestra_client_package/#histogram","text":"","title":"Histogram"},{"location":"learning_orchestra_client_package/#create_histogram","text":"def create_histogram ( self , filename , histogram_filename , fields , pretty_response = True ) filename: filename of file to make histogram histogram_filename: filename used to create histogram fields: list with fields to make histogram pretty_response: return indented string to visualization (default True, if False, return dict)","title":"create_histogram"},{"location":"learning_orchestra_client_package/#modelbuilder","text":"","title":"ModelBuilder"},{"location":"learning_orchestra_client_package/#create_model","text":"create_model ( self , training_filename , test_filename , preprocessor_code , model_classificator , pretty_response = True ) training_filename: filename to be used in training test_filename: filename to be used in test preprocessor_code: python3 code for pyspark preprocessing model model_classificator: list of initial from classificators to be used in model pretty_response: return indented string to visualization (default True, if False, return dict)","title":"create_model"},{"location":"learning_orchestra_client_package/#model_classificator","text":"\"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: create_model ( training_filename , test_filename , preprocessor_code , [ \"lr\" , \"nb\" ])","title":"model_classificator"},{"location":"learning_orchestra_client_package/#preprocessor_code-environment","text":"The python 3 preprocessing code must use the environment instances in bellow: training_df (Instanciated): Spark Dataframe instance for trainingfilename testing_df (Instanciated): Spark Dataframe instance for testing filename The preprocessing code must instanciate the variables in bellow, , all intances must be transformed by pyspark VectorAssembler: features_training (Not Instanciated): Spark Dataframe instance for train the model features_evaluation (Not Instanciated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instanciated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None.","title":"preprocessor_code environment"},{"location":"learning_orchestra_client_package/#handy-methods","text":"self . fields_from_dataframe ( self , dataframe , is_string ) dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields.","title":"Handy methods"},{"location":"learning_orchestra_client_package/#learning_orchestra_client-usage-example","text":"In below there is a python script using the package: from learning_orchestra_client import * cluster_ip = \"34.95.187.26\" Context ( cluster_ip ) database_api = DatabaseApi () print ( database_api . create_file ( \"titanic_training\" , \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" )) print ( database_api . create_file ( \"titanic_testing\" , \"https://filebin.net/mguee52ke97k0x9h/titanic_testing.csv?t=ub4nc1rc\" )) print ( database_api . read_resume_files ()) data_type_handler = DataTypeHandler () print ( data_type_handler . change_file_type ( \"titanic_training\" , { \"Age\" : \"number\" , \"Fare\" : \"number\" , \"Parch\" : \"number\" , \"PassengerId\" : \"number\" , \"Pclass\" : \"number\" , \"SibSp\" : \"number\" , \"Survived\" : \"number\" })) print ( data_type_handler . change_file_type ( \"titanic_testing\" , { \"Age\" : \"number\" , \"Fare\" : \"number\" , \"Parch\" : \"number\" , \"PassengerId\" : \"number\" , \"Pclass\" : \"number\" , \"SibSp\" : \"number\" })) preprocessing_code = ''' from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = ['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don'] correct_initials = ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\"). \\ fit(dataset). \\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Ticket\", \"Cabin\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[1:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) = \\ features_training.randomSplit([0.1, 0.9], seed=11) features_testing = assembler.transform(testing_df) ''' model_builder = Model () print ( model_builder . create_model ( \"titanic_training\" , \"titanic_testing\" , preprocessing_code , [ \"lr\" , \"dt\" , \"gb\" , \"rf\" , \"nb\" ))","title":"learning_orchestra_client usage example"},{"location":"model_builder/","text":"Model builder microservice Model Builder microservice provide a REST API to create several model predictions using your own preprocessing code using a defined set of classificators. Create prediction model POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" } classificators_list \"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] } preprocessor_code environment The python3 preprocessing code must use the environment instances in bellow: training_df (Instanciated): Spark Dataframe instance for trainingfilename testing_df (Instanciated): Spark Dataframe instance for testing filename The preprocessing code must instanciate the variables in bellow, , all intances must be transformed by pyspark VectorAssembler: features_training (Not Instanciated): Spark Dataframe instance for train the model features_evaluation (Not Instanciated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instanciated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None. Handy methods self . fields_from_dataframe ( self , dataframe , is_string ) This method return string or number fields as string list from a dataframe dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields. preprocessor_code example from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Ticket\" , \"Cabin\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [ 1 :], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.1 , 0.9 ], seed = 11 ) features_testing = assembler . transform ( testing_df )","title":"Model Builder"},{"location":"model_builder/#model-builder-microservice","text":"Model Builder microservice provide a REST API to create several model predictions using your own preprocessing code using a defined set of classificators.","title":"Model builder microservice"},{"location":"model_builder/#create-prediction-model","text":"POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" }","title":"Create prediction model"},{"location":"model_builder/#classificators_list","text":"\"lr\": LogisticRegression \"dt\": DecisionTreeClassifier \"rf\": RandomForestClassifier \"gb\": Gradient-boosted tree classifier \"nb\": NaiveBayes to send a request with LogisticRegression and NaiveBayes classificators: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] }","title":"classificators_list"},{"location":"model_builder/#preprocessor_code-environment","text":"The python3 preprocessing code must use the environment instances in bellow: training_df (Instanciated): Spark Dataframe instance for trainingfilename testing_df (Instanciated): Spark Dataframe instance for testing filename The preprocessing code must instanciate the variables in bellow, , all intances must be transformed by pyspark VectorAssembler: features_training (Not Instanciated): Spark Dataframe instance for train the model features_evaluation (Not Instanciated): Spark Dataframe instance for evaluate trained model accuracy features_testing (Not Instanciated): Spark Dataframe instance for test the model Case you don't want evaluate the model prediction, define features_evaluation as None.","title":"preprocessor_code environment"},{"location":"model_builder/#handy-methods","text":"self . fields_from_dataframe ( self , dataframe , is_string ) This method return string or number fields as string list from a dataframe dataframe: dataframe instance is_string: Boolean parameter, if True, the method return the string dataframe fields, otherwise, return the numbers dataframe fields.","title":"Handy methods"},{"location":"model_builder/#preprocessor_code-example","text":"from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Ticket\" , \"Cabin\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [ 1 :], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.1 , 0.9 ], seed = 11 ) features_testing = assembler . transform ( testing_df )","title":"preprocessor_code example"},{"location":"projection/","text":"Projection microservice Projection microservice provide an api to make a projection from file inserted in database service, generating a new file and putting in database Create projection from inserted file POST CLUSTER_IP:5001/projections/<filename> { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Projection"},{"location":"projection/#projection-microservice","text":"Projection microservice provide an api to make a projection from file inserted in database service, generating a new file and putting in database","title":"Projection microservice"},{"location":"projection/#create-projection-from-inserted-file","text":"POST CLUSTER_IP:5001/projections/<filename> { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Create projection from inserted file"},{"location":"usage/","text":"Usage Python package learningOrchestra Client - The python package for learningOrchestra use microservices REST API database api - Microservice used to download and handling files in database projection - Make projections of stored files in database using spark cluster data type handler - Change fields file type between number and text histogram - Make histograms of stored files in database model builder - Create a prediction model from preprocessed files using spark cluster database GUI NoSQLBooster - MongoDB GUI to make several database tasks, as files visualization, querys, projections and files extraction to formats as csv and json, read the database api docs to learn how configure this tool.","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#python-package","text":"learningOrchestra Client - The python package for learningOrchestra use","title":"Python package"},{"location":"usage/#microservices-rest-api","text":"database api - Microservice used to download and handling files in database projection - Make projections of stored files in database using spark cluster data type handler - Change fields file type between number and text histogram - Make histograms of stored files in database model builder - Create a prediction model from preprocessed files using spark cluster","title":"microservices REST API"},{"location":"usage/#database-gui","text":"NoSQLBooster - MongoDB GUI to make several database tasks, as files visualization, querys, projections and files extraction to formats as csv and json, read the database api docs to learn how configure this tool.","title":"database GUI"}]}